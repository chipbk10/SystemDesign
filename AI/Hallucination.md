**AI hallucinations** are when a language model confidently generates false, made-up, or inaccurate information that sounds completely plausible and correct.

It happens because the model predicts words based on patterns in its training data rather than actual facts, so it can "fill in the gaps" with invented details when it lacks real knowledge.
